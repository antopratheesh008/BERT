{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport transformers\nimport pandas as pd \nimport numpy as np\nfrom sklearn import model_selection\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        #self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 30)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, o2 = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n        bo = self.bert_drop(o2)\n        return self.out(bo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTDatasetTraining():\n\n    def __init__(self, qtitle, qbody, answer, targets, tokenizer, max_len):\n        self.qtitle = qtitle\n        self.qbody = qbody\n        self.answer = answer\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.answer)\n    \n    def __getitem__(self,item):\n        question_title = str(self.qtitle[item])\n        question_body = str(self.qbody[item])\n        answer = str(self.answer[item])\n\n        #[CLS] [Q-TITLE] [Q-BODY] [SEP] [ANSWER] [SEP]\n\n        inputs = self.tokenizer.encode_plus(\n            question_title + \" \" + question_body,\n            answer,\n            add_special_tokens=True,\n            max_length = self.max_len,\n            pad_to_max_length = True\n\n        )\n        \n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        #padding_len = self.max_len - len(ids)\n        #ids = ids + ([0] * padding_len)\n        #token_type_ids = token_type_ids + ([0] * padding_len)\n        #mask = mask + ([0] * padding_len)\n\n\n        return {\n            \"ids\":torch.tensor(ids, dtype=torch.long),\n            \"mask\":torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\":torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\" : torch.tensor(self.targets[item, :], dtype=torch.float)\n        }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets)\n\ndef train_loop_fn(data_loader, model, optimizer,device, scheduler = None):\n    print(\"Training the model\")\n    model.train()\n    for bi, d in enumerate(data_loader):\n        ids = d[\"ids\"]\n        mask = d[\"mask\"]\n        token_type_ids = d[\"token_type_ids\"]\n        targets = d[\"targets\"]\n\n        ids= ids.to(device, dtype = torch.long)\n        mask= mask.to(device, dtype = torch.long)\n        token_type_ids= token_type_ids.to(device, dtype = torch.long)\n        targets= targets.to(device, dtype = torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(ids=ids, mask=mask, token_type_ids = token_type_ids)\n        outputs = outputs.to(device)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        if scheduler is not None:\n            scheduler.step()\n        \n        if bi % 10 == 0:\n            print(f\"bi = {bi}, loss = {loss}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_loop(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    for bi, d in enumerate(data_loader):\n        ids = d[\"ids\"]\n        mask = d[\"mask\"]\n        token_type_ids = d[\"token_type_ids\"]\n        targets = d[\"targets\"]\n\n        ids= ids.to(device, dtype = torch.long)\n        mask= mask.to(device, dtype = torch.long)\n        token_type_ids= token_type_ids.to(device, dtype = torch.long)\n        targets= targets.to(device, dtype = torch.float)\n\n        outputs = model(ids=ids, mask=mask, token_type_ids = token_type_ids)\n        loss = loss_fn(outputs, targets)\n\n        fin_targets.append(targets.cpu().detach().numpy())\n        fin_outputs.append(outputs.cpu().detach().numpy())\n\n    return np.vstack(fin_outputs), np.vstack(fin_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run():\n    MAX_LEN=512\n    TRAIN_BATCH_SIZE = 16\n    EPOCHS = 5\n\n    dfx = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\n    df_train, df_valid = model_selection.train_test_split(dfx, random_state=42, test_size=0.1)\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n\n    sample = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\n    target_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n    train_targets = df_train[target_cols].values\n    valid_targets = df_valid[target_cols].values\n\n    tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n    train_dataset = BERTDatasetTraining(\n        qtitle = df_train.question_title.values,\n        qbody = df_train.question_body.values,\n        answer = df_train.answer.values,\n        targets = train_targets,\n        tokenizer = tokenizer,\n        max_len = MAX_LEN\n    )\n\n    print(f\"train_dataset : {len(train_dataset)}\")\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size = TRAIN_BATCH_SIZE,\n        shuffle = False\n    )\n\n    valid_dataset = BERTDatasetTraining(\n        qtitle = df_valid.question_title.values,\n        qbody = df_valid.question_body.values,\n        answer = df_valid.answer.values,\n        targets = valid_targets,\n        tokenizer = tokenizer,\n        max_len = MAX_LEN\n    )\n\n    print(f\"valid_dataset : {len(valid_dataset)}\")\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size = 4,\n        shuffle = False\n    )\n\n    device = \"cuda\"\n    lr = 3e-5\n    num_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS)\n    model = BERTBaseUncased().to(device)\n    optimizer = AdamW(model.parameters(), lr = lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n\n    for epoch in range(EPOCHS):\n        train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n        o, t = eval_loop(valid_data_loader, model, device)\n\n        spear = []\n\n        for jj in range(t.shape[1]):\n            p1 = list(t[:,jj])\n            p2 = list(o[:,jj])\n            coef,_ = np.nan_to_num(stats.spearmanr(p1,p2))\n            spear.append(coef)\n        spear = np.mean(spear)\n        print(f\"epoch = {epoch} , spearman = {spear}\")\n        torch.save(model, \"export.pkl\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model, \"export.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}